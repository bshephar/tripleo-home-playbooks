- include_role:
    name: tripleo_firewall
  name: Run firewall role
  vars:
    tripleo_firewall_engine: iptables
- include_role:
    name: tripleo_iscsid
    tasks_from: iscsid_install.yaml
  name: Iscsid install tasks
- include_role:
    name: tripleo_kernel
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/nova
    setype: container_file_t
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /var/lib/nova
    setype: container_file_t
  - path: /var/lib/_nova_secontext
    setype: container_file_t
  - path: /var/lib/nova/instances
    setype: container_file_t
  - path: /var/lib/libvirt
    setype: container_file_t
- mount:
    fstype: nfs4
    name: /var/lib/nova/instances
    opts: _netdev,bg,{{nfs_options}},nfsvers={{nfs_vers}}
    src: '{{nfs_share}}'
    state: mounted
  name: Mount Nova NFS Share
  vars:
    nfs_backend_enable: false
    nfs_options: context=system_u:object_r:nfs_t:s0
    nfs_share: ''
    nfs_vers: '4.2'
  when: nfs_backend_enable|bool
- name: is Nova Resume Guests State On Host Boot enabled
  set_fact:
    resume_guests_state_on_host_boot_enabled: false
- block:
  - copy:
      content: '[Unit]

        Description=Suspend libvirt Guests in tripleo

        Requires=virt-guest-shutdown.target

        After=systemd-machined.service

        After=network-online.target

        After=tripleo_nova_libvirt.target

        Before=tripleo_nova_compute.service

        Documentation=man:libvirtd(8)

        Documentation=https://libvirt.org


        [Service]

        EnvironmentFile=-/etc/sysconfig/libvirt-guests

        ExecStart=/bin/podman exec nova_libvirt /bin/rm -f /var/lib/libvirt/libvirt-guests

        ExecStop=/bin/podman exec nova_libvirt /bin/sh -x /usr/libexec/libvirt-guests.sh
        shutdown

        Type=oneshot

        RemainAfterExit=yes

        StandardOutput=journal+console

        TimeoutStopSec=0


        [Install]

        WantedBy=multi-user.target

        '
      dest: /etc/systemd/system/tripleo_nova_libvirt_guests.service
    name: libvirt-guests unit to stop nova_compute container before shutdown VMs (monolithic
      libvirt)
    when: tripleo_nova_libvirt_virsh_container | default('nova_libvirt') == 'nova_libvirt'
  - copy:
      content: '[Unit]

        Description=Suspend libvirt Guests in tripleo

        Requires=virt-guest-shutdown.target

        After=systemd-machined.service

        After=network-online.target

        After=tripleo_nova_libvirt.target

        Before=tripleo_nova_compute.service

        Documentation=man:libvirtd(8)

        Documentation=https://libvirt.org


        [Service]

        EnvironmentFile=-/etc/sysconfig/libvirt-guests

        ExecStart=/bin/podman exec nova_virtproxyd /bin/rm -f /var/lib/libvirt/libvirt-guests

        ExecStop=/bin/podman exec nova_virtproxyd /bin/sh -x /usr/libexec/libvirt-guests.sh
        shutdown

        Type=oneshot

        RemainAfterExit=yes

        StandardOutput=journal+console

        TimeoutStopSec=0


        [Install]

        WantedBy=multi-user.target

        '
      dest: /etc/systemd/system/tripleo_nova_libvirt_guests.service
    name: libvirt-guests unit to stop nova_compute container before shutdown VMs (modular
      libvirt)
    when: tripleo_nova_libvirt_virsh_container | default('nova_libvirt') != 'nova_libvirt'
  - copy:
      content: '[Unit]

        Description=Libvirt guests shutdown

        Documentation=https://libvirt.org

        '
      dest: /etc/systemd/system/virt-guest-shutdown.target
    name: Making sure virt-guest-shutdown.target is present
  - name: tripleo_nova_libvirt_guests enable VM shutdown on compute reboot/shutdown
    systemd:
      daemon_reload: true
      enabled: true
      name: tripleo_nova_libvirt_guests
  name: install tripleo_nova_libvirt_guests systemd unit file
  when:
  - resume_guests_state_on_host_boot_enabled|bool
- file:
    path: /var/lib/tripleo-config/ceph
    state: directory
  name: ensure ceph configurations exist
- name: is Instance HA enabled
  set_fact:
    instance_ha_enabled: false
- name: enable virt_sandbox_use_netlink for healthcheck
  seboolean:
    name: virt_sandbox_use_netlink
    persistent: true
    state: true
  when:
  - ansible_facts.selinux is defined
  - ansible_facts.selinux.status == "enabled"
- block:
  - file:
      path: /var/lib/nova/instanceha
      state: directory
    name: prepare Instance HA script directory
  - copy:
      content: "#!/usr/bin/python3\n\nimport os\nimport sys\nimport time\nimport inspect\n\
        import logging\nimport argparse\nimport oslo_config.cfg\nimport requests.exceptions\n\
        \ndef is_forced_down(connection, hostname):\n    services = connection.services.list(host=hostname,\
        \ binary=\"nova-compute\")\n    for service in services:\n        if service.forced_down:\n\
        \            return True\n    return False\n\ndef evacuations_done(connection,\
        \ hostname):\n    # Get a list of migrations.\n    #  :param host: (optional)\
        \ filter migrations by host name.\n    #  :param status: (optional) filter\
        \ migrations by status.\n    #  :param cell_name: (optional) filter migrations\
        \ for a cell.\n    #\n    migrations = connection.migrations.list(host=hostname)\n\
        \n    print(\"Checking %d migrations\" % len(migrations))\n    for migration\
        \ in migrations:\n        # print migration.to_dict()\n        #\n       \
        \ # {\n        # 'status': 'error',\n        # 'dest_host': None,\n      \
        \  # 'new_instance_type_id': 2,\n        # 'old_instance_type_id': 2,\n  \
        \      # 'updated_at': '2018-04-22T20:55:29.000000',\n        # 'dest_compute':\n\
        \        #   'overcloud-novacompute-2.localdomain',\n        # 'migration_type':\
        \ 'live-migration',\n        # 'source_node':\n        #   'overcloud-novacompute-0.localdomain',\n\
        \        # 'id': 8,\n        # 'created_at': '2018-04-22T20:52:58.000000',\n\
        \        # 'instance_uuid':\n        #   'd1c82ce8-3dc5-48db-b59f-854b3b984ef1',\n\
        \        # 'dest_node':\n        #   'overcloud-novacompute-2.localdomain',\n\
        \        # 'source_compute':\n        #   'overcloud-novacompute-0.localdomain'\n\
        \        # }\n        # Acceptable: done, completed, failed\n        if migration.status\
        \ in [\"running\", \"accepted\", \"pre-migrating\"]:\n            return False\n\
        \    return True\n\ndef safe_to_start(connection, hostname):\n    if is_forced_down(connection,\
        \ hostname):\n        print(\"Waiting for fence-down flag to be cleared\"\
        )\n        return False\n    if not evacuations_done(connection, hostname):\n\
        \        print(\"Waiting for evacuations to complete or fail\")\n        return\
        \ False\n    return True\n\ndef create_nova_connection(options):\n    try:\n\
        \        from novaclient import client\n        from novaclient.exceptions\
        \ import NotAcceptable\n    except ImportError:\n        print(\"Nova not\
        \ found or not accessible\")\n        sys.exit(1)\n\n    from keystoneauth1\
        \ import loading\n    from keystoneauth1 import session\n\n    # Prefer the\
        \ oldest and strip the leading 'v'\n    kwargs = dict(\n        auth_url=options[\"\
        auth_url\"][0],\n        username=options[\"username\"][0],\n        password=options[\"\
        password\"][0],\n        project_name=options[\"project_name\"][0],\n    \
        \    user_domain_name=options[\"user_domain_name\"][0],\n        project_domain_name=options[\"\
        project_domain_name\"][0],\n        )\n\n    loader = loading.get_plugin_loader('password')\n\
        \    keystone_auth = loader.load_from_options(**kwargs)\n    keystone_session\
        \ = session.Session(auth=keystone_auth, verify=(not options[\"insecure\"]))\n\
        \n    nova_endpoint_type = 'internalURL'\n    # We default to internalURL\
        \ but we allow this to be overridden via\n    # the [placement]/os_interface\
        \ key.\n    if 'os_interface' in options and len(options[\"os_interface\"\
        ]) == 1:\n        nova_endpoint_type = options[\"os_interface\"][0]\n    #\
        \ Via https://review.opendev.org/#/c/492247/ os_interface has been deprecated\
        \ in queens\n    # and we need to use 'valid_interfaces' which is a:\n   \
        \ # \"List of interfaces, in order of preference, for endpoint URL. (list\
        \ value)\"\n    # Since it is not explicitely set in nova.conf we still keep\
        \ the check for os_interface\n    elif 'valid_interfaces' in options and len(options[\"\
        valid_interfaces\"]) >= 1:\n        nova_endpoint_type = options[\"valid_interfaces\"\
        ][0]\n\n    # This mimicks the code in novaclient/shell.py\n    if nova_endpoint_type\
        \ in ['internal', 'public', 'admin']:\n        nova_endpoint_type += 'URL'\n\
        \n    if 'region_name' in options:\n        region = options['region_name'][0]\n\
        \    elif 'os_region_name' in options:\n        region = options['os_region_name'][0]\n\
        \    else: # We actually try to make a client call even with an empty region\n\
        \        region = None\n    nova_versions = [ \"2.23\", \"2\" ]\n    for version\
        \ in nova_versions:\n        nova = client.Client(version,\n             \
        \                region_name=region,\n                             session=keystone_session,\
        \ auth=keystone_auth,\n                             http_log_debug=\"verbose\"\
        \ in options,\n                             endpoint_type=nova_endpoint_type)\n\
        \n        try:\n            nova.hypervisors.list()\n            return nova\n\
        \n        except NotAcceptable as e:\n            logging.warning(e)\n\n \
        \       except Exception as e:\n            logging.warning(\"Nova connection\
        \ failed. %s: %s\" % (e.__class__.__name__, e))\n\n    print(\"Couldn't obtain\
        \ a supported connection to nova, tried: %s\\n\" % repr(nova_versions))\n\
        \    return None\n\n\nparser = argparse.ArgumentParser(description='Process\
        \ some integers.')\nparser.add_argument('--config-file', dest='nova_config',\
        \ action='store',\n                    default=\"/etc/nova/nova.conf\",\n\
        \                    help='path to nova configuration (default: /etc/nova/nova.conf)')\n\
        parser.add_argument('--nova-binary', dest='nova_binary', action='store',\n\
        \                    default=\"/usr/bin/nova-compute\",\n                \
        \    help='path to nova compute binary (default: /usr/bin/nova-compute)')\n\
        parser.add_argument('--enable-file', dest='enable_file', action='store',\n\
        \                    default=\"/var/lib/nova/instanceha/enabled\",\n     \
        \               help='file exists if instance HA is enabled on this host '\\\
        \n                    '(default: /var/lib/nova/instanceha/enabled)')\n\n\n\
        sections = {}\n(args, remaining) = parser.parse_known_args(sys.argv)\n\nconfig\
        \ = oslo_config.cfg.ConfigParser(args.nova_config, sections)\nconfig.parse()\n\
        config.sections[\"placement\"][\"insecure\"] = 0\nconfig.sections[\"placement\"\
        ][\"verbose\"] = 1\n\nif os.path.isfile(args.enable_file):\n    connection\
        \ = None\n    while not connection:\n        # Loop in case the control plane\
        \ is recovering when we run\n        connection = create_nova_connection(config.sections[\"\
        placement\"])\n        if not connection:\n            time.sleep(10)\n\n\
        \    while not safe_to_start(connection, config.sections[\"DEFAULT\"][\"host\"\
        ][0]):\n        time.sleep(10)\n\nreal_args = [args.nova_binary, '--config-file',\
        \ args.nova_config]\nreal_args.extend(remaining[1:])\nos.execv(args.nova_binary,\
        \ real_args)\n"
      dest: /var/lib/nova/instanceha/check-run-nova-compute
      mode: 493
    name: install Instance HA script that runs nova-compute
  - command: hiera -c /etc/puppet/hiera.yaml compute_instanceha_short_node_names
    name: Get list of instance HA compute nodes
    register: iha_nodes
  - file: path=/var/lib/nova/instanceha/enabled state=touch
    name: If instance HA is enabled on the node activate the evacuation completed
      check
    when: iha_nodes.stdout|lower is search('"'+ansible_facts['hostname']|lower+'"')
  name: install Instance HA recovery script
  when: instance_ha_enabled|bool
- name: Do we prepend nova startup with a delay
  set_fact:
    nova_compute_delay: 0
- copy:
    content: "#!/usr/libexec/platform-python\n\"\"\"\nThis wrapper was created to\
      \ add an optional delay to the startup of nova-compute.\nWe know that instances\
      \ will fail to boot, after a compute reboot, if ceph is not\nhealthy.\n\nIdeally,\
      \ we would poll ceph to get its health, but it's not guaranteed that the\ncompute\
      \ node will have access to the keys.\n\"\"\"\n\nimport os\nimport sys\nimport\
      \ time\nimport logging\nimport argparse\n\nparser = argparse.ArgumentParser(description='Process\
      \ some integers.')\nparser.add_argument('--config-file', dest='nova_config',\
      \ action='store',\n                    default=\"/etc/nova/nova.conf\",\n  \
      \                  help='path to nova configuration (default: /etc/nova/nova.conf)')\n\
      parser.add_argument('--nova-binary', dest='nova_binary', action='store',\n \
      \                   default=\"/usr/bin/nova-compute\",\n                   \
      \ help='path to nova compute binary (default: /usr/bin/nova-compute)')\nparser.add_argument('--delay',\
      \ dest='delay', action='store',\n                    default=120, type=int,\n\
      \                    help='Number of seconds to wait until nova-compute is started')\n\
      parser.add_argument('--state-file', dest='state_file', action='store',\n   \
      \                 default=\"/run/nova-compute-delayed\",\n                 \
      \   help='file exists if we already delayed nova-compute startup'\\\n      \
      \              '(default: /run/nova-compute-delayed)')\n\n\nsections = {}\n\
      (args, remaining) = parser.parse_known_args(sys.argv)\n\nreal_args = [args.nova_binary,\
      \ '--config-file', args.nova_config]\nreal_args.extend(remaining[1:])\n\nif\
      \ not os.path.isfile(args.state_file):\n    logging.info(\"Delaying nova-compute\
      \ startup by %s seconds\" % args.delay)\n    time.sleep(args.delay)\n    open(args.state_file,\
      \ 'a').close()\n\nlogging.info(\"Executing %s\" % real_args)\nos.execv(args.nova_binary,\
      \ real_args)\n"
    dest: /var/lib/nova/delay-nova-compute
    mode: 493
  name: install nova-compute delay wrapper script
  when: nova_compute_delay|int > 0
- name: Is irqbalance enabled
  set_fact:
    compute_irqbalance_disabled: false
- name: disable irqbalance service on compute
  service:
    enabled: false
    name: irqbalance.service
    state: stopped
  when: compute_irqbalance_disabled|bool
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/libvirt
    setype: container_file_t
- file:
    mode: '{{ item.mode | default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype | default(omit) }}'
    state: directory
  name: create libvirt persistent data directories
  with_items:
  - path: /etc/libvirt
    setype: container_file_t
  - path: /etc/libvirt/secrets
    setype: container_file_t
  - path: /etc/libvirt/qemu
    setype: container_file_t
  - path: /var/lib/libvirt
    setype: container_file_t
  - path: /var/cache/libvirt
  - path: /var/lib/nova
    setype: container_file_t
  - path: /run/libvirt
    setype: virt_var_run_t
  - path: /var/log/libvirt
    setype: container_file_t
  - path: /var/log/libvirt/qemu
    setype: container_file_t
  - mode: '0750'
    path: /var/log/containers/libvirt/swtpm
    setype: container_file_t
- group:
    gid: 107
    name: qemu
    state: present
  name: ensure qemu group is present on the host
- name: ensure qemu user is present on the host
  user:
    comment: qemu user
    group: qemu
    name: qemu
    shell: /sbin/nologin
    state: present
    uid: 107
- file:
    group: qemu
    owner: qemu
    path: /var/lib/vhost_sockets
    setype: virt_cache_t
    seuser: system_u
    state: directory
  name: create directory for vhost-user sockets with qemu ownership
- check_mode: false
  command: /usr/bin/rpm -q libvirt-daemon
  failed_when: false
  name: check if libvirt is installed
  register: libvirt_installed
- name: make sure libvirt services are disabled and masked
  service:
    daemon_reload: true
    enabled: false
    masked: true
    name: '{{ item }}'
    state: stopped
  when: libvirt_installed.rc == 0
  with_items:
  - libvirtd.service
  - virtlogd.socket
- copy:
    content: 'd /run/libvirt 0755 root root - -

      '
    dest: /etc/tmpfiles.d/run-libvirt.conf
  name: ensure /run/libvirt is present upon reboot
- name: Enable os_enable_vtpm SELinux boolean for vTPM
  seboolean:
    name: os_enable_vtpm
    persistent: true
    state: true
  when:
  - ansible_facts.selinux is defined
  - ansible_facts.selinux.status == "enabled"
- include_role:
    name: tripleo_nova_migration_target
    tasks_from: install.yaml
  name: nova migration target install task
- name: allow logrotate to read inside containers
  seboolean:
    name: logrotate_read_inside_containers
    persistent: true
    state: true
  when:
  - ansible_facts.selinux is defined
  - ansible_facts.selinux.status == "enabled"
- block:
  - name: Set login facts
    no_log: true
    set_fact:
      tripleo_container_default_pids_limit: 4096
      tripleo_container_events_logger_mechanism: journald
      tripleo_container_registry_login: false
      tripleo_container_registry_logins: {}
      tripleo_container_registry_logins_json: {}
  - name: Convert logins json to dict
    no_log: true
    set_fact:
      tripleo_container_registry_logins: '{{ tripleo_container_registry_logins_json
        | from_json }}'
    when:
    - tripleo_container_registry_logins_json is string
    - tripleo_container_registry_login | bool
    - (tripleo_container_registry_logins_json | length) > 0
  - name: Set registry logins
    no_log: true
    set_fact:
      tripleo_container_registry_logins: '{{ tripleo_container_registry_logins_json
        }}'
    when:
    - tripleo_container_registry_logins_json is mapping
    - tripleo_container_registry_login | bool
    - (tripleo_container_registry_logins_json | length) > 0
  - include_role:
      name: tripleo_podman
      tasks_from: tripleo_podman_install.yml
    name: Run podman install
  - include_role:
      name: tripleo_podman
      tasks_from: tripleo_podman_login.yml
    name: Run podman login
  name: Install and configure Podman
- copy:
    content: 'This file makes tripleo_container_manage generate additional systemd

      dependencies for containers that have special

      start/stop ordering constraints. It ensures that

      those constraints are enforced on reboot/shutdown.

      '
    dest: /etc/sysconfig/podman_drop_in
  name: Configure tripleo_container_manage to generate systemd drop-in dependencies
- include_role:
    name: tripleo_sshd
  vars:
    tripleo_sshd_banner_enabled: false
    tripleo_sshd_banner_text: ''
    tripleo_sshd_message_of_the_day: ''
    tripleo_sshd_motd_enabled: false
    tripleo_sshd_password_authentication: 'no'
    tripleo_sshd_server_options:
      AcceptEnv:
      - LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES
      - LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT
      - LC_IDENTIFICATION LC_ALL LANGUAGE
      - XMODIFIERS
      AuthorizedKeysFile: .ssh/authorized_keys
      ChallengeResponseAuthentication: 'no'
      GSSAPIAuthentication: 'no'
      GSSAPICleanupCredentials: 'no'
      HostKey:
      - /etc/ssh/ssh_host_rsa_key
      - /etc/ssh/ssh_host_ecdsa_key
      - /etc/ssh/ssh_host_ed25519_key
      Subsystem: sftp  /usr/libexec/openssh/sftp-server
      SyslogFacility: AUTHPRIV
      UseDNS: 'no'
      UsePAM: 'yes'
      X11Forwarding: 'yes'
- become: true
  failed_when: false
  name: Check for NTP service
  register: ntp_service_check
  shell: systemctl is-active ntpd.service || systemctl is-enabled ntpd.service
- name: Disable NTP before configuring Chrony
  service:
    enabled: false
    name: ntpd
    state: stopped
  when:
  - ntp_service_check.rc is defined
  - ntp_service_check.rc == 0
- include_role:
    name: chrony
  name: Install, Configure and Run Chrony
- command: chronyc makestep
  name: Force NTP sync
- command: chronyc waitsync 30
  name: Ensure system is NTP time synced
- include_role:
    name: tripleo_timezone
  name: Run timezone role
  vars:
    tripleo_timezone: UTC
- include_role:
    name: tuned
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/openvswitch
    setype: container_file_t
  - path: /var/lib/openvswitch/ovn
    setype: container_file_t
- copy:
    content: "#!/usr/bin/bash\n# Cleanup neutron OVS bridges. To be called on startup\
      \ to avoid\n# \"difficult-to-debug\" issues with partially configured resources.\n\
      \nNEUTRON_OVS_CONF=/var/lib/config-data/puppet-generated/neutron/etc/neutron/plugins/ml2/openvswitch_agent.ini\n\
      \nif [ -e ${NEUTRON_OVS_CONF} ];\nthen\n    INT_BRIDGE=`crudini --get ${NEUTRON_OVS_CONF}\
      \ ovs integration_bridge`\n    TUN_BRIDGE=`crudini --get ${NEUTRON_OVS_CONF}\
      \ ovs tunnel_bridge`\nfi\n\nfor port in `ovs-vsctl list-ports ${INT_BRIDGE:-\"\
      br-int\"}`;\ndo\n    skip_cleanup=`ovs-vsctl --if-exists get Interface $port\
      \ external_ids:skip_cleanup`\n    if ! [[ \"x$skip_cleanup\" == \"x\\\"true\\\
      \"\" ]];\n    then\n        ovs-vsctl del-port ${INT_BRIDGE:-\"br-int\"} $port\n\
      \    fi\ndone\n\novs-vsctl --if-exists del-br ${TUN_BRIDGE:-\"br-tun\"}\n\n\
      # Clean up trunk port bridges\nfor br in $(ovs-vsctl list-br | egrep 'tbr-[0-9a-f\\\
      -]+'); do\n    ovs-vsctl --if-exists del-br $br\ndone\n"
    dest: /usr/libexec/neutron-cleanup
    force: true
    mode: '0755'
  name: Copy in cleanup script
- copy:
    content: '[Unit]

      Description=Neutron cleanup on startup

      After=openvswitch.service network.target

      Before=tripleo_neutron_ovs_agent.service tripleo_neutron_dhcp.service tripleo_neutron_l3_agent.service
      tripleo_nova_compute.service

      RefuseManualStop=yes


      [Service]

      Type=oneshot

      ExecStart=/usr/libexec/neutron-cleanup


      [Install]

      WantedBy=multi-user.target

      '
    dest: /usr/lib/systemd/system/neutron-cleanup.service
    force: true
  name: Copy in cleanup service
- name: Enabling the cleanup service
  service:
    enabled: true
    name: neutron-cleanup
  when: not (ansible_check_mode|bool)
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/neutron
    setype: container_file_t
- command: ip netns add ns_temp
  failed_when: false
  name: create /run/netns with temp namespace
  register: ipnetns_add_result
- command: ip netns delete ns_temp
  failed_when: false
  name: remove temp namespace
  when:
  - ipnetns_add_result.rc is defined
  - ipnetns_add_result.rc == 0
- file:
    path: /var/lib/neutron
    setype: container_file_t
    state: directory
  name: create /var/lib/neutron
- name: set conditions
  set_fact:
    debug_enabled: false
    haproxy_wrapper_enabled: true
- file:
    path: /var/lib/neutron/kill_scripts
    state: directory
  name: create kill_scripts directory within /var/lib/neutron
- copy:
    content: "#!/bin/bash\n{% if debug_enabled|bool -%}\nset -x\n{% endif -%}\nadd_date()\
      \ {\n  echo \"$(date) $@\"\n}\n\n# Set up script logging for debugging purpose.\n\
      # It will be taken care of by logrotate since there is the .log\n# suffix.\n\
      exec 3>&1 4>&2\ntrap 'exec 2>&4 1>&3' 0 1 2 3\nexec 1>>/var/log/neutron/kill-script.log\
      \ 2>&1\n\nSIG=$1\nPID=$2\nNETNS=$(ip netns identify ${PID})\n\nif [ \"x${NETNS}\"\
      \ == \"x\" ]; then\n  CLI=\"nsenter --all --preserve-credentials -t 1 podman\"\
      \n  SIG=9\nelse\n  CLI=\"nsenter --net=/run/netns/${NETNS} --preserve-credentials\
      \ -m -t 1 podman\"\nfi\n\nkill_container() {\n  add_date \"Stopping container\
      \ $1 ($2)\"\n  $CLI stop $2\n  delete_container $1 $2\n}\n\nsignal_container()\
      \ {\n  SIGNAL=$3\n  if [ -z \"$SIGNAL\" ]; then\n      SIGNAL=\"HUP\"\n  fi\n\
      \  add_date \"Sending signal '$SIGNAL' to $1 ($2)\"\n  $CLI kill --signal $SIGNAL\
      \ $2\n}\n\ndelete_container() {\n  add_date \"Deleting container $1 ($2)\"\n\
      \  $CLI rm $2 || echo \"Deleting container $1 ($2) failed\"\n}\n\n\n{% raw -%}\n\
      if [ -f /proc/$PID/cgroup ]; then\n  # Get container ID based on process cgroups\n\
      \  CT_ID=$(awk 'BEGIN {FS=\".scope|-\"} /0::|:pids:/ {if ($(NF-1)) print $(NF-1);exit}'\
      \ /proc/$PID/cgroup)\n  CT_NAME=$($CLI inspect -f '{{.Name}}' $CT_ID)\n\n  case\
      \ $SIG in\n    HUP)\n      signal_container $CT_NAME $CT_ID\n      ;;\n    9)\n\
      \      kill_container $CT_NAME $CT_ID\n      ;;\n    15)\n      signal_container\
      \ $CT_NAME $CT_ID 15\n      delete_container $CT_NAME $CT_ID\n      ;;\n   \
      \ *)\n      add_date \"Unknown action ${SIG} for ${CT_NAME} ${CT_ID}\"\n   \
      \   exit 1\n      ;;\n  esac\n\nelse\n  add_date \"No such PID: ${PID}\"\n \
      \ exit 1\nfi\n{% endraw %}\n"
    dest: /var/lib/neutron/kill_scripts/haproxy-kill
    mode: 493
  name: create haproxy kill script
  when: haproxy_wrapper_enabled|bool
