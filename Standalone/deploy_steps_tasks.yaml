- block:
  - file:
      path: '{{ item }}'
      serole: object_r
      setype: cert_t
      seuser: system_u
      state: directory
    name: Create dirs for certificates and keys
    with_items:
    - /etc/pki/tls/certs/httpd
    - /etc/pki/tls/private/httpd
  - include_role:
      name: linux-system-roles.certificate
    vars:
      certificate_requests:
      - ca: ipa
        dns: '{{ fqdn_ctlplane }}'
        key_size: '2048'
        name: httpd-ctlplane
        principal: HTTP/{{ fqdn_ctlplane }}@{{ idm_realm }}
        run_after: 'cp /etc/pki/tls/certs/httpd-ctlplane.crt /etc/pki/tls/certs/httpd/httpd-ctlplane.crt

          cp /etc/pki/tls/private/httpd-ctlplane.key /etc/pki/tls/private/httpd/httpd-ctlplane.key

          pkill -USR1 httpd

          '
      - ca: ipa
        dns: '{{ fqdn_storage }}'
        key_size: '2048'
        name: httpd-storage
        principal: HTTP/{{ fqdn_storage }}@{{ idm_realm }}
        run_after: 'cp /etc/pki/tls/certs/httpd-storage.crt /etc/pki/tls/certs/httpd/httpd-storage.crt

          cp /etc/pki/tls/private/httpd-storage.key /etc/pki/tls/private/httpd/httpd-storage.key

          pkill -USR1 httpd

          '
      - ca: ipa
        dns: '{{ fqdn_storage_mgmt }}'
        key_size: '2048'
        name: httpd-storage_mgmt
        principal: HTTP/{{ fqdn_storage_mgmt }}@{{ idm_realm }}
        run_after: 'cp /etc/pki/tls/certs/httpd-storage_mgmt.crt /etc/pki/tls/certs/httpd/httpd-storage_mgmt.crt

          cp /etc/pki/tls/private/httpd-storage_mgmt.key /etc/pki/tls/private/httpd/httpd-storage_mgmt.key

          pkill -USR1 httpd

          '
      - ca: ipa
        dns: '{{ fqdn_internal_api }}'
        key_size: '2048'
        name: httpd-internal_api
        principal: HTTP/{{ fqdn_internal_api }}@{{ idm_realm }}
        run_after: 'cp /etc/pki/tls/certs/httpd-internal_api.crt /etc/pki/tls/certs/httpd/httpd-internal_api.crt

          cp /etc/pki/tls/private/httpd-internal_api.key /etc/pki/tls/private/httpd/httpd-internal_api.key

          pkill -USR1 httpd

          '
      - ca: ipa
        dns: '{{ fqdn_external }}'
        key_size: '2048'
        name: httpd-external
        principal: HTTP/{{ fqdn_external }}@{{ idm_realm }}
        run_after: 'cp /etc/pki/tls/certs/httpd-external.crt /etc/pki/tls/certs/httpd/httpd-external.crt

          cp /etc/pki/tls/private/httpd-external.key /etc/pki/tls/private/httpd/httpd-external.key

          pkill -USR1 httpd

          '
  name: Certificate generation
  when:
  - step|int == 1
  - enable_internal_tls
- import_role:
    name: tripleo_container_tag
  name: Cinder Volume tag container image for pacemaker
  vars:
    container_image: quay.io/tripleomastercentos9/openstack-cinder-volume:current-tripleo
    container_image_latest: quay.io/tripleomastercentos9/openstack-cinder-volume:pcmklatest
  when: step|int == 1
- block:
  - import_role:
      name: tripleo_ha_wrapper
    name: Cinder volume puppet bundle
    vars:
      tripleo_ha_wrapper_bundle_name: openstack-cinder-volume
      tripleo_ha_wrapper_puppet_config_volume: cinder
      tripleo_ha_wrapper_puppet_debug: false
      tripleo_ha_wrapper_puppet_execute: include ::tripleo::profile::base::pacemaker;
        include ::tripleo::profile::pacemaker::cinder::volume_bundle
      tripleo_ha_wrapper_puppet_tags: pacemaker::resource::bundle,pacemaker::property,pacemaker::resource::ip,pacemaker::resource::ocf,pacemaker::constraint::order,pacemaker::constraint::colocation
      tripleo_ha_wrapper_resource_name: openstack-cinder-volume
      tripleo_ha_wrapper_resource_state: _ Started
      tripleo_ha_wrapper_service_name: cinder_volume
  name: Cinder Volume HA Wrappers Step
  when: step|int == 5
- block:
  - copy:
      content: "#!/bin/sh\ntmpwatch --nodirs \\\n  -X \"/var/log/containers/*/*log\"\
        \ \\\n  -X \"/var/log/containers/*/*/*log\" \\\n  -X \"/var/log/containers/*/*err\"\
        \ \\\n  {{ LogrotatePurgeAfterDays|int +1 }}d \\\n  /var/log/containers/ 2>&1\
        \ | logger -t container-tmpwatch\n"
      dest: /usr/local/sbin/containers-tmpwatch
      group: root
      mode: 493
      owner: root
    name: Push script
    vars:
      LogrotatePurgeAfterDays: '14'
  - cron:
      job: /usr/local/sbin/containers-tmpwatch
      name: Remove old logs
      special_time: daily
      user: root
    name: Insert cronjob in root crontab
  name: configure tmpwatch on the host
  when: step|int == 2
- block:
  - name: Check if rsyslog exists
    register: rsyslog_config
    shell: systemctl is-active rsyslog
  - block:
    - blockinfile:
        content: 'if $syslogfacility-text == ''{{facility}}'' and $programname ==
          ''haproxy'' then -/var/log/containers/haproxy/haproxy.log

          & stop

          '
        create: true
        path: /etc/rsyslog.d/openstack-haproxy.conf
      name: Forward logging to haproxy.log file
      register: logconfig
      vars:
        facility: local0
    - name: restart rsyslog service after logging conf change
      service:
        name: rsyslog
        state: restarted
      when: logconfig is changed
    when:
    - rsyslog_config is changed
    - rsyslog_config.rc == 0
  name: Configure rsyslog for HAproxy container managed by Pacemaker
  when: step|int == 1
- block:
  - failed_when: ( ( "self signed certificate" not in openssl_output.stderr ) and
      ( "OK" not in openssl_output.stdout ) ) or ("expired" in openssl_output.stderr)
    name: Verify SSL certificate
    register: openssl_output
    shell: 'cat << EOF | openssl verify

      {{ssl_cert}}

      EOF

      '
    when:
    - ( ssl_cert | length ) > 512
    - protocol == "https"
  - fail:
      msg: 'SSLCertificate is empty or too short and PublicSSLCertificateAutogenerated
        is False and at least one endpoint is configured with https

        '
    when:
    - ( ssl_cert | length ) < 512
    - not ( auto_gen | bool )
    - protocol == "https"
  name: Validate SSLCertificate is properly defined if PublicSSLCertificateAutogenerated
    is False
  vars:
    auto_gen: false
    protocol: https
    ssl_cert: "-----BEGIN CERTIFICATE-----\nMIIEBjCCAu6gAwIBAgIUD9dWy9NTo7VdQIVgWrskiCtqzxcwDQYJKoZIhvcNAQEL\n\
      BQAwXTELMAkGA1UEBhMCQVUxEzARBgNVBAgMClF1ZWVuc2xhbmQxETAPBgNVBAcM\nCEJyaXNiYW5lMREwDwYDVQQKDAhibmUtaG9tZTETMBEGA1UEAwwKVHJpcGxlTyBD\n\
      QTAeFw0yMjAzMzAwNDIwNDVaFw0yMzA4MTIwNDIwNDVaMIGdMQswCQYDVQQGEwJB\nVTETMBEGA1UECAwKUXVlZW5zbGFuZDERMA8GA1UEBwwIQnJpc2JhbmUxETAPBgNV\n\
      BAoMCHlvdXItb3JnMQ4wDAYDVQQLDAVhZG1pbjEiMCAGCSqGSIb3DQEJARYTYnNo\nZXBoYXJAcmVkaGF0LmNvbTEfMB0GA1UEAwwWb3BlbnN0YWNrLmJuZS1ob21lLm5l\n\
      dDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMdykKsu74rVca3fc0uw\n/l9CtKjdQBtutnPFUgMqeqI70pP4ap/13BjqjKCo9teheyhZlkzDUfd6UAwIBFqW\n\
      rZt+Dd2PfuS/MtUyw1kvdYmh7Lpaqi4XBcORFty/jBnO7NucpkyvszwNNs0W+H8g\nZ1JMUzdJ8UtNlMZT86cQ15YSTjye0S/ntZGOHbiMPoGeQOXzP+vz8hXKscY+w7uv\n\
      Px9HsLgdmXFipZ1smbgGm5vubtlSI3fNoLFI92AMGiPLSdF48GS7ydVOGh6BMzdg\nGozrlyia/v7gnKEvDvmzIp/eAb6ZUv4ThMmwpi4mR7BqxXNZ/IitTd4ppm+0IRcC\n\
      q70CAwEAAaN9MHswHwYDVR0jBBgwFoAUD99iTxSeV1pImIWdV/nBRD8jJ2EwCQYD\nVR0TBAIwADALBgNVHQ8EBAMCBPAwIQYDVR0RBBowGIIWb3BlbnN0YWNrLmJuZS1o\n\
      b21lLm5ldDAdBgNVHQ4EFgQUq5b3jd2e+Nj5qWO/2lCOM9TZRb0wDQYJKoZIhvcN\nAQELBQADggEBAJzmJRDSEllG+2pA/QD73mbde79G6BcIhOxOLsCzG6bqYKWtEgEJ\n\
      kGu6EsHlkN6TMFiwME8dZu5yGJZQ8hZxg6yXjJfgUQB23w6zxcLGE5W93h0bYrvI\ntHDiFELe+NtlOqJ38TYCfi6Or3l0Yh8ZzZSXrk39D0L619hcVPb8T6niM2f7pG76\n\
      2DRsU0A+56iQIxZZhUPh7jbiDy/NePqf1dCRnoR3423qq8sEW/jnCKWs2dq0RWD6\n+i1ULnJfTAvSSpET1jd/FJ3VTaa2441j5MneOxrptEayx92UaZZ+gbwEyr62nq4M\n\
      EHP8xqG4qn7lz0BpsFbBb+ZyCVcG6Zyt5xM=\n-----END CERTIFICATE-----  \n"
  when:
  - true
  - step|int == 2
- import_role:
    name: tripleo_container_tag
  name: HAproxy tag container image for pacemaker
  vars:
    container_image: quay.io/tripleomastercentos9/openstack-haproxy:current-tripleo
    container_image_latest: quay.io/tripleomastercentos9/openstack-haproxy:pcmklatest
  when: step|int == 1
- block:
  - import_role:
      name: tripleo_ha_wrapper
    name: HAproxy puppet bundle
    vars:
      tripleo_ha_wrapper_bundle_name: haproxy-bundle
      tripleo_ha_wrapper_puppet_config_volume: haproxy
      tripleo_ha_wrapper_puppet_debug: false
      tripleo_ha_wrapper_puppet_execute: include ::tripleo::profile::base::pacemaker;
        include ::tripleo::profile::pacemaker::haproxy_bundle
      tripleo_ha_wrapper_puppet_tags: pacemaker::resource::bundle,pacemaker::property,pacemaker::resource::ip,pacemaker::resource::ocf,pacemaker::constraint::order,pacemaker::constraint::colocation
      tripleo_ha_wrapper_resource_name: haproxy-bundle
      tripleo_ha_wrapper_resource_state: Started
      tripleo_ha_wrapper_service_name: haproxy
  name: HAproxy HA Wrappers Step
  when: step|int == 2
- include_role:
    name: tripleo_lvmfilter
  name: Run lvmfilter role
  when:
  - step|int == 1
- containers.podman.podman_container_info:
    name: keystone
  delay: 30
  failed_when:
  - keystone_infos.containers.0.Healthcheck.Status is defined
  - '''healthy'' not in keystone_infos.containers.0.Healthcheck.Status'
  name: validate keystone container state
  register: keystone_infos
  retries: 10
  tags:
  - opendev-validation
  - opendev-validation-keystone
  when:
  - not container_healthcheck_disabled
  - step|int == 4
- include_role:
    name: tripleo_keystone
    tasks_from: keystone-db-sync.yaml
  name: Keystone DB sync
  when:
  - step|int == 3
- import_role:
    name: tripleo_keystone
    tasks_from: keystone.yaml
  name: Keystone containers
  when:
  - step|int == 3
- import_role:
    name: tripleo_keystone
    tasks_from: keystone-bootstrap.yaml
  name: Keystone bootstrap containers
  vars:
    tripleo_keystone_admin_password: 7i8YsihaRsvMJKyBxT2Z2RD3a
    tripleo_keystone_admin_url: http://192.168.1.25:35357
    tripleo_keystone_internal_url: http://192.168.1.25:5000
    tripleo_keystone_public_url: https://openstack.bne-home.net:13000
    tripleo_keystone_region: regionOne
  when:
  - step|int == 3
- block:
  - include_role:
      name: linux-system-roles.certificate
    vars:
      certificate_requests:
      - ca: ipa
        dns:
        - '{{fqdn_internal_api}}'
        - '{{cloud_names.cloud_name_internal_api}}'
        key_size: '2048'
        name: mysql
        principal: mysql/{{fqdn_internal_api}}@{{idm_realm}}
  name: Certificate generation
  when:
  - step|int == 1
  - enable_internal_tls
- import_role:
    name: tripleo_container_tag
  name: MySQL tag container image for pacemaker
  vars:
    container_image: quay.io/tripleomastercentos9/openstack-mariadb:current-tripleo
    container_image_latest: quay.io/tripleomastercentos9/openstack-mariadb:pcmklatest
  when: step|int == 1
- block:
  - import_role:
      name: tripleo_ha_wrapper
    name: Mysql puppet bundle
    vars:
      tripleo_ha_wrapper_bundle_name: galera-bundle
      tripleo_ha_wrapper_puppet_config_volume: mysql
      tripleo_ha_wrapper_puppet_debug: false
      tripleo_ha_wrapper_puppet_execute: '["Mysql_datadir", "Mysql_user", "Mysql_database",
        "Mysql_grant", "Mysql_plugin"].each |String $val| { noop_resource($val) };
        include ::tripleo::profile::base::pacemaker; include ::tripleo::profile::pacemaker::database::mysql_bundle'
      tripleo_ha_wrapper_puppet_tags: pacemaker::resource::bundle,pacemaker::property,pacemaker::resource::ocf,pacemaker::constraint::order,pacemaker::constraint::colocation
      tripleo_ha_wrapper_resource_name: galera
      tripleo_ha_wrapper_resource_state: Master
      tripleo_ha_wrapper_service_name: mysql
  name: MySQL HA Wrappers Step
  when: step|int == 2
- block:
  - become: true
    failed_when: false
    name: Retrieve the galera container
    register: galera_bundle_id
    shell: '{{ container_cli }} ps -q --filter name=galera-bundle'
  - become: true
    name: Update credentials in the container
    shell: '{{ container_cli }} exec -u root -t ''{{ galera_bundle_id.stdout_lines[0]
      }}'' cp /var/lib/kolla/config_files/src/root/.my.cnf /root'
    when: galera_bundle_id.rc == 0 and galera_bundle_id.stdout != ""
  name: Sync MySQL credentials in the running container
  when: step|int == 3
- containers.podman.podman_container_info:
    name: nova_api
  delay: 30
  failed_when:
  - nova_api_infos.containers.0.Healthcheck.Status is defined
  - '''healthy'' not in nova_api_infos.containers.0.Healthcheck.Status'
  name: validate nova-api container state
  register: nova_api_infos
  retries: 10
  tags:
  - opendev-validation
  - opendev-validation-nova
  when:
  - not container_healthcheck_disabled
  - step|int == 4
- containers.podman.podman_container_info:
    name: nova_compute
  delay: 30
  failed_when:
  - nova_compute_infos.containers.0.Healthcheck.Status is defined
  - '''healthy'' not in nova_compute_infos.containers.0.Healthcheck.Status'
  name: validate nova-compute container state
  register: nova_compute_infos
  retries: 10
  tags:
  - opendev-validation
  - opendev-validation-nova
  when:
  - not container_healthcheck_disabled
  - step|int == 6
- include_role:
    name: tripleo_nvdimm
  name: manage PMEM namespaces for vPMEM
  vars:
    tripleo_nvdimm_pmem_namespaces: ''
  when:
  - step|int == 1
  - tripleo_nvdimm_pmem_namespaces != ''
- block:
  - name: is KSM enabled
    set_fact:
      compute_ksm_enabled: false
  - block:
    - become: true
      failed_when: false
      name: Check for ksm
      register: ksm_service_check
      shell: systemctl is-active ksm.service || systemctl is-enabled ksm.service
    - name: disable KSM services
      register: ksmdisabled
      service:
        enabled: false
        name: '{{ item }}'
        state: stopped
      when:
      - ksm_service_check.rc is defined
      - ksm_service_check.rc == 0
      with_items:
      - ksm.service
      - ksmtuned.service
    - command: echo 2 >/sys/kernel/mm/ksm/run
      name: delete PageKSM after disable ksm on compute
      when:
      - ksm_service_check.rc is defined
      - ksm_service_check.rc == 0
      - ksmdisabled is changed
    name: disable KSM on compute
    when: not compute_ksm_enabled|bool
  - block:
    - name: make sure package providing ksmtuned is installed (RHEL8 or CentOS8)
      package:
        name: qemu-kvm-common
        state: present
      when:
      - ansible_facts['distribution_major_version'] is version('8', '==')
    - name: enable ksmtunded
      service:
        enabled: true
        name: '{{ item }}'
        state: started
      with_items:
      - ksm.service
      - ksmtuned.service
    name: enable KSM on compute
    when: compute_ksm_enabled|bool
  name: enable/disable ksm
  when:
  - step|int == 1
- containers.podman.podman_container_info:
    name: nova_conductor
  delay: 30
  failed_when:
  - nova_conductor_infos.containers.0.Healthcheck.Status is defined
  - '''healthy'' not in nova_conductor_infos.containers.0.Healthcheck.Status'
  name: validate nova-conductor container state
  register: nova_conductor_infos
  retries: 10
  tags:
  - opendev-validation
  - opendev-validation-nova
  when:
  - not container_healthcheck_disabled
  - step|int == 5
- block:
  - become: true
    copy:
      content: '[Unit]

        Wants=tripleo_nova_virtsecretd.service

        Wants=tripleo_nova_virtnodedevd.service

        Wants=tripleo_nova_virtstoraged.service

        Wants=tripleo_nova_virtproxyd.service

        Wants=tripleo_nova_virtqemud.service

        After=tripleo_nova_virtsecretd.service

        After=tripleo_nova_virtnodedevd.service

        After=tripleo_nova_virtstoraged.service

        After=tripleo_nova_virtproxyd.service

        After=tripleo_nova_virtqemud.service

        '
      dest: /etc/systemd/system/tripleo_nova_libvirt.target
      group: root
      mode: '0644'
      owner: root
    name: Create systemd file
    register: libvirt_target_result
  - become: true
    name: Reload systemd
    systemd:
      daemon_reload: true
      enabled: true
      name: tripleo_nova_libvirt.target
      state: restarted
    when: libvirt_target_result.changed
  name: Set up systemd target for libvirt services
  when: step|int == 4
- containers.podman.podman_container_info:
    name: nova_metadata
  delay: 30
  failed_when:
  - nova_metadata_infos.containers.0.Healthcheck.Status is defined
  - '''healthy'' not in nova_metadata_infos.containers.0.Healthcheck.Status'
  name: validate nova-metadata container state
  register: nova_metadata_infos
  retries: 10
  tags:
  - opendev-validation
  - opendev-validation-nova
  when:
  - not container_healthcheck_disabled
  - step|int == 5
- containers.podman.podman_container_info:
    name: nova_migration_target
  delay: 30
  failed_when:
  - nova_migration_target_infos.containers.0.Healthcheck.Status is defined
  - '''healthy'' not in nova_migration_target_infos.containers.0.Healthcheck.Status'
  name: validate nova-migration-target container state
  register: nova_migration_target_infos
  retries: 10
  tags:
  - opendev-validation
  - opendev-validation-nova
  when:
  - not container_healthcheck_disabled
  - step|int == 5
- containers.podman.podman_container_info:
    name: nova_scheduler
  delay: 30
  failed_when:
  - nova_scheduler_infos.containers.0.Healthcheck.Status is defined
  - '''healthy'' not in nova_scheduler_infos.containers.0.Healthcheck.Status'
  name: validate nova-scheduler container state
  register: nova_scheduler_infos
  retries: 10
  tags:
  - opendev-validation
  - opendev-validation-nova
  when:
  - not container_healthcheck_disabled
  - step|int == 5
- containers.podman.podman_container_info:
    name: nova_vnc_proxy
  delay: 30
  failed_when:
  - nova_vnc_proxy_infos.containers.0.Healthcheck.Status is defined
  - '''healthy'' not in nova_vnc_proxy_infos.containers.0.Healthcheck.Status'
  name: validate nova-vnc-proxy container state
  register: nova_vnc_proxy_infos
  retries: 10
  tags:
  - opendev-validation
  - opendev-validation-nova
  when:
  - not container_healthcheck_disabled
  - step|int == 5
- block:
  - include_role:
      name: linux-system-roles.certificate
    vars:
      certificate_requests:
      - ca: ipa
        dns: '{{fqdn_internal_api}}'
        key_size: '2048'
        name: ovn_controller
        principal: ovn_controller/{{fqdn_internal_api}}@{{idm_realm}}
  name: Certificate generation
  when:
  - step|int == 1
  - enable_internal_tls
- import_role:
    name: tripleo_container_tag
  name: OVN DBS tag container image for pacemaker
  vars:
    container_image: quay.io/tripleomastercentos9/openstack-ovn-northd:current-tripleo
    container_image_latest: quay.io/tripleomastercentos9/openstack-ovn-northd:pcmklatest
  when: step|int == 1
- block:
  - import_role:
      name: tripleo_ha_wrapper
    name: Ovn dbs puppet bundle
    vars:
      tripleo_ha_wrapper_bundle_name: ovn-dbs-bundle
      tripleo_ha_wrapper_puppet_config_volume: ovn_dbs
      tripleo_ha_wrapper_puppet_debug: false
      tripleo_ha_wrapper_puppet_execute: include ::tripleo::profile::base::pacemaker;
        include ::tripleo::profile::pacemaker::ovn_dbs_bundle
      tripleo_ha_wrapper_puppet_tags: pacemaker::resource::bundle,pacemaker::property,pacemaker::resource::ip,pacemaker::resource::ocf,pacemaker::constraint::order,pacemaker::constraint::colocation
      tripleo_ha_wrapper_resource_name: ovndb_servers
      tripleo_ha_wrapper_resource_state: Slave Master
      tripleo_ha_wrapper_service_name: ovn_dbs
  name: OVNDbs HA Wrappers Step
  when: step|int == 3
- block:
  - include_role:
      name: linux-system-roles.certificate
    vars:
      certificate_requests:
      - ca: ipa
        dns: '{{fqdn_internal_api}}'
        key_size: '2048'
        name: ovn_dbs
        principal: ovn_dbs/{{fqdn_internal_api}}@{{idm_realm}}
  name: Certificate generation
  when:
  - step|int == 1
  - enable_internal_tls
- block:
  - include_role:
      name: linux-system-roles.certificate
    vars:
      certificate_requests:
      - ca: ipa
        dns: '{{fqdn_internal_api}}'
        key_size: '2048'
        name: ovn_metadata
        principal: ovn_metadata/{{fqdn_internal_api}}@{{idm_realm}}
  name: Certificate generation
  when:
  - step|int == 1
  - enable_internal_tls
- block:
  - include_role:
      name: linux-system-roles.certificate
    vars:
      certificate_requests:
      - ca: ipa
        dns: '{{fqdn_internal_api}}'
        key_size: '2048'
        name: rabbitmq
        principal: rabbitmq/{{fqdn_internal_api}}@{{idm_realm}}
        run_after: "container_name=$({{container_cli}} ps --format=\\{\\{.Names\\\
          }\\} | grep -w -E 'rabbitmq(-bundle-.*-[0-9]+)?')\nservice_crt=\"/etc/pki/tls/certs/rabbitmq.crt\"\
          \nservice_key=\"/etc/pki/tls/private/rabbitmq.key\"\nif echo \"$container_name\"\
          \ | grep -q \"^rabbitmq-bundle\"; then\n  # lp#1917868: Do not use podman\
          \ cp with HA containers as they get\n  # frozen temporarily and that can\
          \ make pacemaker operation fail.\n  tar -c \"$service_crt\" \"$service_key\"\
          \ | {{container_cli}} exec -i \"$container_name\" tar -C / -xv\n  # no need\
          \ to update the mount point, because pacemaker\n  # recreates the container\
          \ when it's restarted\nelse\n  # Refresh the cert at the mount-point\n \
          \ {{container_cli}} cp $service_crt \"$container_name:/var/lib/kolla/config_files/src-tls/$service_crt\"\
          \n  # Refresh the key at the mount-point\n  {{container_cli}} cp $service_key\
          \ \"$container_name:/var/lib/kolla/config_files/src-tls/$service_key\"\n\
          \  # Copy the new cert from the mount-point to the real path\n  {{container_cli}}\
          \ exec -u root \"$container_name\" cp \"/var/lib/kolla/config_files/src-tls$service_crt\"\
          \ \"$service_crt\"\n  # Copy the new key from the mount-point to the real\
          \ path\n  {{container_cli}} exec -u root \"$container_name\" cp \"/var/lib/kolla/config_files/src-tls$service_key\"\
          \ \"$service_key\"\nfi\n# Set appropriate permissions\n{{container_cli}}\
          \ exec -u root \"$container_name\" chown rabbitmq:rabbitmq \"$service_crt\"\
          \n{{container_cli}} exec -u root \"$container_name\" chown rabbitmq:rabbitmq\
          \ \"$service_key\"\n# Trigger a pem cache clear in RabbitMQ to read the\
          \ new certificates\n{{container_cli}} exec \"$container_name\" rabbitmqctl\
          \ eval \"ssl:clear_pem_cache().\"\n"
  name: Certificate generation
  when:
  - step|int == 1
  - enable_internal_tls
- import_role:
    name: tripleo_container_tag
  name: RabbitMQ tag container image for pacemaker
  vars:
    container_image: quay.io/tripleomastercentos9/openstack-rabbitmq:current-tripleo
    container_image_latest: quay.io/tripleomastercentos9/openstack-rabbitmq:pcmklatest
  when: step|int == 1
- block:
  - import_role:
      name: tripleo_ha_wrapper
    name: Rabbitmq rpc puppet bundle
    vars:
      tripleo_ha_wrapper_bundle_name: rabbitmq-bundle
      tripleo_ha_wrapper_puppet_config_volume: rabbitmq
      tripleo_ha_wrapper_puppet_debug: false
      tripleo_ha_wrapper_puppet_execute: '["Rabbitmq_policy", "Rabbitmq_user"].each
        |String $val| { noop_resource($val) }; include ::tripleo::profile::base::pacemaker;
        include ::tripleo::profile::pacemaker::rabbitmq_bundle'
      tripleo_ha_wrapper_puppet_tags: pacemaker::resource::bundle,pacemaker::property,pacemaker::resource::ip,pacemaker::resource::ocf,pacemaker::constraint::order,pacemaker::constraint::colocation
      tripleo_ha_wrapper_resource_name: rabbitmq
      tripleo_ha_wrapper_resource_state: Started
      tripleo_ha_wrapper_service_name: oslo_messaging_rpc
  name: RabbitMQ RPC HA Wrappers Step
  when: step|int == 2
- block:
  - become: true
    containers.podman.podman_image:
      force: true
      name: '{{ prefetch_image }}'
      validate_certs: false
    delay: 5
    loop: '{{ lookup(''template'', tripleo_role_name + ''/docker_config.yaml'', errors=''ignore'')
      | default(''{}'', True) | from_yaml | recursive_get_key_from_dict(key=''image'')
      | unique }}'
    loop_control:
      loop_var: prefetch_image
    name: Pre-fetch all the containers
    register: result
    retries: 5
    until: result is succeeded
  when:
  - (step|int) == 1
- include_role:
    name: tripleo_container_manage
    tasks_from: shutdown.yml
  name: Manage tripleo container services
  when:
  - (step|int) == 1
- block:
  - include_role:
      name: linux-system-roles.certificate
    vars:
      certificate_requests:
      - ca: ipa
        dns:
        - '{{fqdn_internal_api}}'
        - '{{cloud_names.cloud_name_internal_api}}'
        key_size: '2048'
        name: redis
        principal: redis/{{fqdn_internal_api}}@{{idm_realm}}
        run_after: 'container_name=$({{container_cli}} ps --format=\{\{.Names\}\}
          | grep redis_tls_proxy)

          service_crt="/etc/pki/tls/certs/redis.crt"

          service_key="/etc/pki/tls/private/redis.key"

          # Copy the new cert from the mount-point to the real path

          {{container_cli}} exec "$container_name" cp "/var/lib/kolla/config_files/src-tls$service_crt"
          "$service_crt"

          # Copy the new cert from the mount-point to the real path

          {{container_cli}} exec "$container_name" cp "/var/lib/kolla/config_files/src-tls$service_key"
          "$service_key"

          # Set appropriate permissions

          {{container_cli}} exec "$container_name" chown memcached:memcached "$service_crt"

          {{container_cli}} exec "$container_name" chown memcached:memcached "$service_key"

          # Trigger a reload for stunnel to read the new certificate

          {{container_cli}} exec pkill -o -HUP stunnel

          '
  name: Certificate generation
  when:
  - step|int == 1
  - enable_internal_tls
- block:
  - name: Check if rsyslog exists
    register: swift_rsyslog_config
    shell: systemctl is-active rsyslog
  - copy:
      content: '# Fix for https://bugs.launchpad.net/tripleo/+bug/1776180

        local2.*                 /var/log/containers/swift/swift.log

        &                        stop

        '
      dest: /etc/rsyslog.d/openstack-swift.conf
    name: Forward logging to swift.log file
    register: swift_logconfig
    when:
    - swift_rsyslog_config is defined
    - swift_rsyslog_config.rc == 0
  - name: Restart rsyslogd service after logging conf change
    service:
      name: rsyslog
      state: restarted
    when:
    - swift_logconfig is defined
    - swift_logconfig is changed
  name: Configure rsyslog for swift
  when:
  - step|int == 1
- become: true
  failed_when:
  - kolla_set_configs_result.rc is defined
  - kolla_set_configs_result.rc not in [0, 125]
  name: Run kolla_set_configs to copy ring files
  register: kolla_set_configs_result
  shell: '{{ container_cli }} exec -u root {{ item }} /usr/local/bin/kolla_set_configs '
  when: step|int == 5
  with_items:
  - swift_proxy
- become: true
  failed_when:
  - kolla_set_configs_result.rc is defined
  - kolla_set_configs_result.rc not in [0, 125]
  name: Run kolla_set_configs to copy ring files
  register: kolla_set_configs_result
  shell: '{{ container_cli }} exec -u root {{ item }} /usr/local/bin/kolla_set_configs'
  when: step|int == 5
  with_items:
  - swift_account_auditor
  - swift_account_reaper
  - swift_account_replicator
  - swift_account_server
  - swift_container_auditor
  - swift_container_replicator
  - swift_container_server
  - swift_container_updater
  - swift_object_auditor
  - swift_object_expirer
  - swift_object_replicator
  - swift_object_server
  - swift_object_updater
  - swift_recon_cron
